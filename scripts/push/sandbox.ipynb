{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1975791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener_historico_ofertas_stock.py\n",
    "# Script para replicar datos de ventas de STOCK y OFERTAS desde SQL Server a PostgreSQL usando psycopg2 y Prefect.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import psycopg2 as pg2\n",
    "from psycopg2.extras import execute_values\n",
    "import logging\n",
    "from prefect import flow, task, get_run_logger\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configurar logging\n",
    "logger = logging.getLogger(\"replicacion_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "file_handler = logging.FileHandler(\"logs/replicacion_psycopg2.log\", encoding=\"utf-8\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "SQL_SERVER = os.getenv(\"SQL_SERVER\")\n",
    "SQL_USER = os.getenv(\"SQL_USER\")\n",
    "SQL_PASSWORD = os.getenv(\"SQL_PASSWORD\")\n",
    "SQL_DATABASE = os.getenv(\"SQL_DATABASE\")\n",
    "PG_HOST = os.getenv(\"PG_HOST\")\n",
    "PG_PORT = os.getenv(\"PG_PORT\")\n",
    "PG_DB = os.getenv(\"PG_DB\")\n",
    "PG_USER = os.getenv(\"PG_USER\")\n",
    "PG_PASSWORD = os.getenv(\"PG_PASSWORD\")\n",
    "\n",
    "# Crear engine SQL Server\n",
    "def open_sql_conn():\n",
    "    print(f\"Conectando a SQL Server: {SQL_SERVER}\")\n",
    "    print(f\"Conectando a SQL Server: {SQL_DATABASE}\") \n",
    "    return create_engine(f\"mssql+pyodbc://{SQL_USER}:{SQL_PASSWORD}@{SQL_SERVER}/{SQL_DATABASE}?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "\n",
    "def open_pg_conn():\n",
    "    return pg2.connect(dbname=PG_DB, user=PG_USER, password=PG_PASSWORD, host=PG_HOST, port=PG_PORT)\n",
    "\n",
    "def infer_postgres_types(df):\n",
    "    type_map = {\n",
    "        \"int64\": \"BIGINT\",\n",
    "        \"int32\": \"INTEGER\",\n",
    "        \"float64\": \"DOUBLE PRECISION\",\n",
    "        \"bool\": \"BOOLEAN\",\n",
    "        \"datetime64[ns]\": \"TIMESTAMP\",\n",
    "        \"object\": \"TEXT\"\n",
    "    }\n",
    "    col_defs = [f\"{col} {type_map.get(str(df[col].dtype), 'TEXT')}\" for col in df.columns]\n",
    "    return \", \".join(col_defs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53251ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Generando datos de STOCK para ID: 20\n",
      "Conectando a SQL Server: 10.54.200.92\n",
      "Conectando a SQL Server: data-sync\n"
     ]
    }
   ],
   "source": [
    "ids = 20\n",
    "print(f\"-> Generando datos de STOCK para ID: {ids}\")\n",
    "\n",
    "# Cargar STOCK por Proveedor\n",
    "\"\"\"Consulta el stock y devuelve un dict {fecha: cantidad}, limitado a fechas válidas hasta ayer.\"\"\"\n",
    "conn = open_sql_conn()\n",
    "query_stock = f\"\"\"\n",
    "    SELECT  \n",
    "        S.[C_ANIO],S.[C_MES],S.[C_SUCU_EMPR],S.[C_ARTICULO], A.[C_PROVEEDOR_PRIMARIO]\n",
    "        ,S.[Q_DIA1],S.[Q_DIA2],S.[Q_DIA3],S.[Q_DIA4],S.[Q_DIA5],S.[Q_DIA6],S.[Q_DIA7],S.[Q_DIA8],S.[Q_DIA9],S.[Q_DIA10]\n",
    "        ,S.[Q_DIA11],S.[Q_DIA12],S.[Q_DIA13],S.[Q_DIA14],S.[Q_DIA15],S.[Q_DIA16],S.[Q_DIA17],S.[Q_DIA18],S.[Q_DIA19],S.[Q_DIA20]\n",
    "        ,S.[Q_DIA21],S.[Q_DIA22],S.[Q_DIA23],S.[Q_DIA24],S.[Q_DIA25],S.[Q_DIA26],S.[Q_DIA27],S.[Q_DIA28],S.[Q_DIA29],S.[Q_DIA30],S.[Q_DIA31]  \n",
    "    FROM [repl].[T710_ESTADIS_STOCK] S\n",
    "    LEFT JOIN [repl].[T050_ARTICULOS] A  \n",
    "        ON S.C_ARTICULO = A.C_ARTICULO  \n",
    "    WHERE C_ANIO = 2025\n",
    "    AND A.C_PROVEEDOR_PRIMARIO IN ({ids});\n",
    "\"\"\"\n",
    "df_stock = pd.read_sql(query_stock, conn) # type: ignore\n",
    "conn.dispose()  # Cerrar conexión SQL Server\n",
    "\n",
    "if df_stock.empty:\n",
    "    print(f\"⚠️ No se encontraron datos de stock para el proveedor {id_proveedor} en el mes actual.\") # type: ignore\n",
    "    # return {}\n",
    "df_stock['C_ANIO'] = df_stock['C_ANIO'].astype(int)\n",
    "df_stock['C_MES'] = df_stock['C_MES'].astype(int)\n",
    "df_stock['C_ARTICULO'] = df_stock['C_ARTICULO'].astype(int)\n",
    "df_stock['C_SUCU_EMPR'] = df_stock['C_SUCU_EMPR'].astype(int)\n",
    "df_stock['C_PROVEEDOR_PRIMARIO'] = df_stock['C_PROVEEDOR_PRIMARIO'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660533ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_PROVEEDOR_PRIMARIO  C_ARTICULO  C_SUCU_EMPR       FECHA  STOCK\n",
      "0                    20         166            1  2025-01-01    0.0\n",
      "1                    20        3796            1  2025-01-01    0.0\n",
      "2                    20        3797            1  2025-01-01    0.0\n",
      "3                    20        4770            1  2025-01-01    0.0\n",
      "4                    20        4771            1  2025-01-01    0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Función para verificar si la fecha es válida\n",
    "def es_fecha_valida(anio, mes, dia):\n",
    "    try:\n",
    "        return date(anio, mes, dia)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Suponiendo que df_oferta es tu DataFrame original\n",
    "# Creamos un DataFrame vacío para ir apilando las filas transformadas\n",
    "df_trf_stock = pd.DataFrame()\n",
    "\n",
    "# Iteramos sobre las columnas de ofertas\n",
    "for col in df_stock.columns:\n",
    "    if col.startswith(\"Q_DIA\"):\n",
    "        # Extraemos el número del día de la columna (por ejemplo, 'M_OFERTA_DIA1' -> 1)\n",
    "        dia = int(col.replace(\"Q_DIA\", \"\"))\n",
    "        \n",
    "        # Copiamos los datos necesarios para este día\n",
    "        df_tmp = df_stock[['C_ANIO', 'C_MES', 'C_SUCU_EMPR', 'C_PROVEEDOR_PRIMARIO', 'C_ARTICULO', col]].copy()\n",
    "        df_tmp['DIA'] = dia\n",
    "        \n",
    "        # Creamos la fecha a partir del año, mes y día\n",
    "        df_tmp['FECHA'] = df_tmp.apply(lambda row: es_fecha_valida(int(row['C_ANIO']), int(row['C_MES']), int(row['DIA'])), axis=1)\n",
    "        # El valor float es el de la columna de stock diario, por ejemplo: df_tmp[col]\n",
    "        # En este contexto, df_tmp[col] contiene valores float64 (por ejemplo, 0.0, 464.0, etc.)\n",
    "        # Si quieres crear una columna con el valor de stock, puedes hacer:\n",
    "        df_tmp['STOCK'] = df_tmp[col].astype(float)   \n",
    "        \n",
    "        # Filtramos fechas inválidas\n",
    "        df_tmp = df_tmp.dropna(subset=['FECHA'])\n",
    "        \n",
    "        # Filtramos fechas futuras (solo fechas hasta el día anterior)\n",
    "        df_tmp = df_tmp[df_tmp['FECHA'] <= (datetime.now().date() - timedelta(days=1))]\n",
    "\n",
    "        # Creamos la columna de 'OFERTA' (1 si tiene oferta, 0 si no)\n",
    "        df_tmp['OFERTA'] = df_tmp[col].apply(lambda x: 1 if x == 'S' else 0)\n",
    "        \n",
    "        # Ahora solo conservamos las columnas relevantes\n",
    "        df_tmp = df_tmp[['C_PROVEEDOR_PRIMARIO','C_ARTICULO', 'C_SUCU_EMPR', 'FECHA', 'STOCK']]\n",
    "        \n",
    "        # Agregamos al DataFrame final\n",
    "        df_trf_stock = pd.concat([df_trf_stock, df_tmp], ignore_index=True)\n",
    "\n",
    "# Mostrar el resultado final\n",
    "print(df_trf_stock.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad4e3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_PROVEEDOR_PRIMARIO  C_ARTICULO  C_SUCU_EMPR       FECHA  STOCK\n",
      "0                    20         166            1  2025-01-31    0.0\n",
      "1                    20        3796            1  2025-01-31  722.0\n",
      "2                    20        3797            1  2025-01-31  453.0\n",
      "3                    20        4770            1  2025-01-31   43.0\n",
      "4                    20        4771            1  2025-01-31    0.0\n"
     ]
    }
   ],
   "source": [
    "print(df_tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7f85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a SQL Server: 10.54.200.92\n",
      "Conectando a SQL Server: data-sync\n"
     ]
    }
   ],
   "source": [
    "# Cargar OFERTAS por Proveedor\n",
    "\"\"\"Consulta el OFERTAS y devuelve un dict {fecha: flag}, limitado a fechas válidas hasta ayer.\"\"\"\n",
    "conn = open_sql_conn()\n",
    "query_oferta = f\"\"\"\n",
    "    SELECT  \n",
    "        O.C_ANIO, O.C_MES, O.C_SUCU_EMPR, O.C_ARTICULO, A.C_PROVEEDOR_PRIMARIO,\n",
    "        O.M_OFERTA_DIA1, O.M_OFERTA_DIA2, O.M_OFERTA_DIA3, O.M_OFERTA_DIA4, O.M_OFERTA_DIA5, \n",
    "        O.M_OFERTA_DIA6, O.M_OFERTA_DIA7, O.M_OFERTA_DIA8, O.M_OFERTA_DIA9, O.M_OFERTA_DIA10,\n",
    "        O.M_OFERTA_DIA11, O.M_OFERTA_DIA12, O.M_OFERTA_DIA13, O.M_OFERTA_DIA14, O.M_OFERTA_DIA15, \n",
    "        O.M_OFERTA_DIA16, O.M_OFERTA_DIA17, O.M_OFERTA_DIA18, O.M_OFERTA_DIA19, O.M_OFERTA_DIA20,\n",
    "        O.M_OFERTA_DIA21, O.M_OFERTA_DIA22, O.M_OFERTA_DIA23, O.M_OFERTA_DIA24, O.M_OFERTA_DIA25, \n",
    "        O.M_OFERTA_DIA26, O.M_OFERTA_DIA27, O.M_OFERTA_DIA28, O.M_OFERTA_DIA29, O.M_OFERTA_DIA30,\n",
    "        O.M_OFERTA_DIA31\n",
    "    FROM [repl].[T710_ESTADIS_OFERTA_FOLDER] O\n",
    "    LEFT JOIN [repl].[T050_ARTICULOS] A ON O.C_ARTICULO = A.C_ARTICULO\n",
    "    WHERE C_ANIO = 2025\n",
    "    AND A.C_PROVEEDOR_PRIMARIO IN ({ids});\n",
    "\"\"\"\n",
    "df_oferta = pd.read_sql(query_oferta, conn) # type: ignore\n",
    "conn.dispose()  # Cerrar conexión SQL Server\n",
    "if df_stock.empty:\n",
    "    print(f\"⚠️ No se encontraron datos de stock para el proveedor {id_proveedor} en el mes actual.\") # type: ignore\n",
    "    # return {}\n",
    "df_oferta['C_ANIO'] = df_oferta['C_ANIO'].astype(int)\n",
    "df_oferta['C_MES'] = df_oferta['C_MES'].astype(int)\n",
    "df_oferta['C_ARTICULO'] = df_oferta['C_ARTICULO'].astype(int)\n",
    "df_oferta['C_SUCU_EMPR'] = df_oferta['C_SUCU_EMPR'].astype(int)\n",
    "df_oferta['C_PROVEEDOR_PRIMARIO'] = df_oferta['C_PROVEEDOR_PRIMARIO'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00fc4df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_PROVEEDOR_PRIMARIO  C_ARTICULO  C_SUCU_EMPR       FECHA  OFERTA\n",
      "0                    20       31872           79  2025-04-01       0\n",
      "1                    20       31873           79  2025-04-01       0\n",
      "2                    20       31874           79  2025-04-01       0\n",
      "3                    20       33091           79  2025-04-01       0\n",
      "4                    20       33092           79  2025-04-01       0\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import date, datetime, timedelta\n",
    "\n",
    "# # Función para verificar si la fecha es válida\n",
    "# def es_fecha_valida(anio, mes, dia):\n",
    "#     try:\n",
    "#         return date(anio, mes, dia)\n",
    "#     except ValueError:\n",
    "#         return None\n",
    "\n",
    "# Suponiendo que df_oferta es tu DataFrame original\n",
    "# Creamos un DataFrame vacío para ir apilando las filas transformadas\n",
    "df_transformado = pd.DataFrame()\n",
    "\n",
    "# Iteramos sobre las columnas de ofertas\n",
    "for col in df_oferta.columns:\n",
    "    if col.startswith(\"M_OFERTA_DIA\"):\n",
    "        # Extraemos el número del día de la columna (por ejemplo, 'M_OFERTA_DIA1' -> 1)\n",
    "        dia = int(col.replace(\"M_OFERTA_DIA\", \"\"))\n",
    "        \n",
    "        # Copiamos los datos necesarios para este día\n",
    "        df_tmp = df_oferta[['C_ANIO', 'C_MES', 'C_SUCU_EMPR', 'C_PROVEEDOR_PRIMARIO', 'C_ARTICULO', col]].copy()\n",
    "        df_tmp['DIA'] = dia\n",
    "        \n",
    "        # Creamos la fecha a partir del año, mes y día\n",
    "        df_tmp['FECHA'] = df_tmp.apply(lambda row: es_fecha_valida(row['C_ANIO'], row['C_MES'], row['DIA']), axis=1)\n",
    "        \n",
    "        # Filtramos fechas inválidas\n",
    "        df_tmp = df_tmp.dropna(subset=['FECHA'])\n",
    "        \n",
    "        # Filtramos fechas futuras (solo fechas hasta el día anterior)\n",
    "        df_tmp = df_tmp[df_tmp['FECHA'] <= (datetime.now().date() - timedelta(days=1))]\n",
    "\n",
    "        # Creamos la columna de 'OFERTA' (1 si tiene oferta, 0 si no)\n",
    "        df_tmp['OFERTA'] = df_tmp[col].apply(lambda x: 1 if x == 'S' else 0)\n",
    "        \n",
    "        # Ahora solo conservamos las columnas relevantes\n",
    "        df_tmp = df_tmp[['C_PROVEEDOR_PRIMARIO','C_ARTICULO', 'C_SUCU_EMPR', 'FECHA', 'OFERTA']]\n",
    "        \n",
    "        # Agregamos al DataFrame final\n",
    "        df_transformado = pd.concat([df_transformado, df_tmp], ignore_index=True)\n",
    "\n",
    "# Mostrar el resultado final\n",
    "print(df_transformado.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e32c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_PROVEEDOR_PRIMARIO  C_ARTICULO  C_SUCU_EMPR       FECHA  OFERTA   STOCK\n",
      "0                    20       31872           79  2025-04-01       0   727.0\n",
      "1                    20       31873           79  2025-04-01       0  1230.0\n",
      "2                    20       31874           79  2025-04-01       0    27.0\n",
      "3                    20       33091           79  2025-04-01       0   128.0\n",
      "4                    20       33092           79  2025-04-01       0   113.0\n"
     ]
    }
   ],
   "source": [
    "# Realizamos el merge para agregar las columnas de df_transformado_stock a df_transformado\n",
    "df_combined = pd.merge(df_transformado, df_trf_stock[['C_PROVEEDOR_PRIMARIO','C_ARTICULO', 'C_SUCU_EMPR', 'FECHA', 'STOCK']], \n",
    "                    on=['C_PROVEEDOR_PRIMARIO','C_ARTICULO', 'C_SUCU_EMPR', 'FECHA'], how='left')\n",
    "\n",
    "# Mostrar el resultado final\n",
    "print(df_combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000ab1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_actual = datetime.now().date()\n",
    "fecha_limite = fecha_actual - timedelta(days=60)\n",
    "df_filtered = df_combined[df_combined['FECHA'] >= fecha_limite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809dc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb638efc",
   "metadata": {},
   "source": [
    "## SUBIR DATOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d871fbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertados 50000 registros en el bloque 1\n",
      "Insertados 50000 registros en el bloque 2\n",
      "Insertados 50000 registros en el bloque 3\n",
      "Insertados 50000 registros en el bloque 4\n",
      "Insertados 50000 registros en el bloque 5\n",
      "Insertados 50000 registros en el bloque 6\n",
      "Insertados 50000 registros en el bloque 7\n",
      "Insertados 50000 registros en el bloque 8\n",
      "Insertados 50000 registros en el bloque 9\n",
      "Insertados 50000 registros en el bloque 10\n",
      "Insertados 50000 registros en el bloque 11\n",
      "Insertados 50000 registros en el bloque 12\n",
      "Insertados 50000 registros en el bloque 13\n",
      "Insertados 50000 registros en el bloque 14\n",
      "Insertados 50000 registros en el bloque 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dtype_map.get(\u001b[38;5;28mstr\u001b[39m(series.dtype), \u001b[33m'\u001b[39m\u001b[33mTEXT\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Llamada a la función para subir el DataFrame a PostgreSQL\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mupload_to_postgres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mupload_to_postgres\u001b[39m\u001b[34m(df_combined, table_name, chunk_size)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_combined), chunk_size)):\n\u001b[32m     25\u001b[39m     values_chunk = [\u001b[38;5;28mtuple\u001b[39m(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df_combined.iloc[chunk:chunk+chunk_size].itertuples(index=\u001b[38;5;28;01mFalse\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mexecute_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minsert_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     conn.commit()\n\u001b[32m     28\u001b[39m     total_rows += \u001b[38;5;28mlen\u001b[39m(values_chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\ETL\\prefect\\Lib\\site-packages\\psycopg2\\extras.py:1299\u001b[39m, in \u001b[36mexecute_values\u001b[39m\u001b[34m(cur, sql, argslist, template, page_size, fetch)\u001b[39m\n\u001b[32m   1297\u001b[39m     parts.append(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1298\u001b[39m parts[-\u001b[32m1\u001b[39m:] = post\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fetch:\n\u001b[32m   1301\u001b[39m     result.extend(cur.fetchall())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\utf_8.py:15\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(input, errors)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m### Codec APIs\u001b[39;00m\n\u001b[32m     13\u001b[39m encode = codecs.utf_8_encode\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28minput\u001b[39m, errors=\u001b[33m'\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs.utf_8_decode(\u001b[38;5;28minput\u001b[39m, errors, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIncrementalEncoder\u001b[39;00m(codecs.IncrementalEncoder):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "\n",
    "def upload_to_postgres(df_combined, table_name=\"src.historico_ofertas_stock\", chunk_size=50000):\n",
    "    # Conectar a la base de datos\n",
    "    with open_pg_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Crear la tabla si no existe\n",
    "        columns = ', '.join(df_combined.columns)\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {', '.join([f\"{col} {infer_postgres_type(df_combined[col])}\" for col in df_combined.columns])}\n",
    "        );\n",
    "        \"\"\"\n",
    "        cur.execute(create_sql)\n",
    "\n",
    "        # Insertar los datos en la tabla en bloques (chunks)\n",
    "        insert_sql = f\"INSERT INTO {table_name} ({columns}) VALUES %s\"\n",
    "        total_rows = 0\n",
    "\n",
    "        for i, chunk in enumerate(range(0, len(df_combined), chunk_size)):\n",
    "            values_chunk = [tuple(row) for row in df_combined.iloc[chunk:chunk+chunk_size].itertuples(index=False, name=None)]\n",
    "            execute_values(cur, insert_sql, values_chunk)\n",
    "            conn.commit()\n",
    "            total_rows += len(values_chunk)\n",
    "            print(f\"Insertados {len(values_chunk)} registros en el bloque {i+1}\")\n",
    "\n",
    "        # Cerrar cursor\n",
    "        cur.close()\n",
    "\n",
    "def infer_postgres_type(series):\n",
    "    \"\"\"\n",
    "    Inferir el tipo de datos PostgreSQL para cada columna del DataFrame\n",
    "    \"\"\"\n",
    "    dtype_map = {\n",
    "        'int64': 'BIGINT',\n",
    "        'float64': 'FLOAT',\n",
    "        'object': 'TEXT',\n",
    "        'datetime64[ns]': 'TIMESTAMP'\n",
    "    }\n",
    "    return dtype_map.get(str(series.dtype), 'TEXT')\n",
    "\n",
    "# Llamada a la función para subir el DataFrame a PostgreSQL\n",
    "upload_to_postgres(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5def0ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d4592f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a SQL Server: 10.54.200.92\n",
      "Conectando a SQL Server: data-sync\n"
     ]
    }
   ],
   "source": [
    "# Cargar PRECIOS por Proveedor\n",
    "\"\"\"Consulta el PRECIOS y devuelve un dict {fecha: flag}, limitado a fechas válidas hasta ayer.\"\"\"\n",
    "conn = open_sql_conn()\n",
    "query_precios = f\"\"\"\n",
    "    SELECT  \n",
    "        P.C_ANIO, P.C_MES,A.C_PROVEEDOR_PRIMARIO, P.C_ARTICULO, P.C_SUCU_EMPR, P.I_PRECIO_VTA_1, P.I_PRECIO_VTA_2, P.I_PRECIO_VTA_3, P.I_PRECIO_VTA_4, P.I_PRECIO_VTA_5 \n",
    "        ,P.I_PRECIO_VTA_6, P.I_PRECIO_VTA_7, P.I_PRECIO_VTA_8, P.I_PRECIO_VTA_9, P.I_PRECIO_VTA_10, P.I_PRECIO_VTA_11, P.I_PRECIO_VTA_12, P.I_PRECIO_VTA_13 \n",
    "        ,P.I_PRECIO_VTA_14, P.I_PRECIO_VTA_15, P.I_PRECIO_VTA_16, P.I_PRECIO_VTA_17, P.I_PRECIO_VTA_18, P.I_PRECIO_VTA_19, P.I_PRECIO_VTA_20, P.I_PRECIO_VTA_21  \n",
    "        ,P.I_PRECIO_VTA_22, P.I_PRECIO_VTA_23, P.I_PRECIO_VTA_24, P.I_PRECIO_VTA_25, P.I_PRECIO_VTA_26, P.I_PRECIO_VTA_27, P.I_PRECIO_VTA_28, P.I_PRECIO_VTA_29 \n",
    "        ,P.I_PRECIO_VTA_30, P.I_PRECIO_VTA_31 \n",
    "    FROM [repl].[T710_ESTADIS_PRECIOS] P\n",
    "    LEFT JOIN [repl].[T050_ARTICULOS] A \n",
    "\tON P.C_ARTICULO = A.C_ARTICULO\n",
    "    WHERE P.C_ANIO = 2025\n",
    "    AND A.C_PROVEEDOR_PRIMARIO IN ({ids});\n",
    "\"\"\"\n",
    "df_precios = pd.read_sql(query_precios, conn) # type: ignore\n",
    "conn.dispose()  # Cerrar conexión SQL Server\n",
    "if df_stock.empty:\n",
    "    print(f\"⚠️ No se encontraron datos de stock para el proveedor {id_proveedor} en el mes actual.\") # type: ignore\n",
    "    # return {}\n",
    "df_precios['C_ANIO'] = df_precios['C_ANIO'].astype(int)\n",
    "df_precios['C_MES'] = df_precios['C_MES'].astype(int)\n",
    "df_precios['C_ARTICULO'] = df_precios['C_ARTICULO'].astype(int)\n",
    "df_precios['C_SUCU_EMPR'] = df_precios['C_SUCU_EMPR'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Unir ambos DataFrames por las columnas comunes\n",
    "    df = pd.merge(df_stock, df_oferta, on=['C_ANIO', 'C_MES', 'C_SUCU_EMPR', 'C_ARTICULO'], how='outer')\n",
    "    df.fillna(0, inplace=True)  # Rellenar NaN con 0 para las columnas de stock y ofertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "def convertir_stock_diario_a_dict(df_stock):\n",
    "    \"\"\"Convierte df_stock en un diccionario {fecha: cantidad}, solo hasta ayer y con fechas válidas.\"\"\"\n",
    "    def es_fecha_valida(anio, mes, dia):\n",
    "        try:\n",
    "            return date(anio, mes, dia)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    resultado = {}\n",
    "    for _, row in df_stock.iterrows():\n",
    "        anio = int(row['C_ANIO'])\n",
    "        mes = int(row['C_MES'])\n",
    "\n",
    "        for col in df_stock.columns:\n",
    "            if col.startswith(\"Q_DIA\"):\n",
    "                dia = int(col.replace(\"Q_DIA\", \"\"))\n",
    "                fecha_valida = es_fecha_valida(anio, mes, dia)\n",
    "                if fecha_valida and fecha_valida <= (datetime.now().date() - timedelta(days=1)):\n",
    "                    resultado[fecha_valida.isoformat()] = row[col]\n",
    "    return resultado\n",
    "\n",
    "# BLOQUE AGREGADO PARA INCORPORAR OFERTAS en formato DICCIONARIO\n",
    "def convertir_ofertas_a_dict(df_ofertas):\n",
    "    \"\"\"Convierte df_ofertas en un diccionario {fecha: flag}, solo hasta ayer y con fechas válidas.\"\"\"\n",
    "    def es_fecha_valida(anio, mes, dia):\n",
    "        try:\n",
    "            return date(anio, mes, dia)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    resultado = {}\n",
    "    for _, row in df_ofertas.iterrows():\n",
    "        anio = int(row['C_ANIO'])\n",
    "        mes = int(row['C_MES'])\n",
    "\n",
    "        for col in df_ofertas.columns:\n",
    "            if col.startswith(\"M_OFERTA_DIA\"):\n",
    "                dia = int(col.replace(\"M_OFERTA_DIA\", \"\"))\n",
    "                fecha_valida = es_fecha_valida(anio, mes, dia)\n",
    "                if fecha_valida and fecha_valida <= (datetime.now().date() - timedelta(days=1)):\n",
    "                    resultado[fecha_valida.isoformat()] = row[col]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_stock = convertir_stock_diario_a_dict(df_stock)\n",
    "datos_ofertas = convertir_ofertas_a_dict(df_oferta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stock.columns)\n",
    "df_oferta.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
