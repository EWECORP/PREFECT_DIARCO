{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee8b951",
   "metadata": {},
   "source": [
    "## Pruebas de Consolidación de OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea680a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import psycopg2 as pg2\n",
    "\n",
    "# Cargar configuración DINAMICA de acuerdo al entorno\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "import io\n",
    "\n",
    "ENV_PATH = os.environ.get(\"ETL_ENV_PATH\", \"C:/ETL/ETL_DIARCO/.env\")  # Toma Producción si está definido, o la ruta por defecto E:\\ETL\\ETL_DIARCO\\.env\n",
    "# Verificar si el archivo .env existe\n",
    "if not os.path.exists(ENV_PATH):\n",
    "    print(f\"El archivo .env no existe en la ruta: {ENV_PATH}\")\n",
    "    print(f\"Directorio actual: {os.getcwd()}\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "secrets = dotenv_values(ENV_PATH)\n",
    "folder = f\"C:/ETL/ETL_DIARCO/{secrets['FOLDER_DATOS']}\"\n",
    "folder_logs = f\"./pruebas/logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73f389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones Locales\n",
    "def Open_Connection():\n",
    "    conn_str = f'DRIVER={secrets[\"SQLP_DRIVER\"]};SERVER={secrets[\"SQLP_SERVER\"]};PORT={secrets[\"SQLP_PORT\"]};DATABASE={secrets[\"SQLP_DATABASE\"]};UID={secrets[\"SQLP_USER\"]};PWD={secrets[\"SQLP_PASSWORD\"]}'\n",
    "    # print (conn_str) \n",
    "    try:    \n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        return conn\n",
    "    except:\n",
    "        print('Error en la Conexión')\n",
    "        return None\n",
    "\n",
    "def Open_Diarco_Data(): \n",
    "    conn_str = f\"dbname={secrets['PG_DB']} user={secrets['PG_USER']} password={secrets['PG_PASSWORD']} host={secrets['PG_HOST']} port={secrets['PG_PORT']}\"\n",
    "    #print (conn_str)\n",
    "    for i in range(5):\n",
    "        try:    \n",
    "            conn = pg2.connect(conn_str)\n",
    "            return conn\n",
    "        except Exception as e:\n",
    "            print(f'Error en la conexión: {e}')\n",
    "            time.sleep(5)\n",
    "    return None  # Retorna None si todos los intentos fallan\n",
    "\n",
    "def Close_Connection(conn): \n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        # print(\"[OK] Conexión cerrada.\")    \n",
    "    return True\n",
    "\n",
    "os.makedirs(folder_logs, exist_ok=True)\n",
    "log_file = os.path.join(folder_logs, \"publicacion_oc_precarga.log\")\n",
    "\n",
    "#\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Función para limpiar y normalizar los campos\n",
    "def limpiar_campos_oc(df):\n",
    "    # Normalizar textos respetando la longitud máxima del destino\n",
    "    df[\"c_usuario_genero_oc\"]   = df[\"c_usuario_genero_oc\"].fillna(\"\").astype(str).str[:10]\n",
    "    df[\"c_terminal_genero_oc\"]  = df[\"c_terminal_genero_oc\"].fillna(\"\").astype(str).str[:15]\n",
    "    df[\"c_usuario_bloqueo\"]     = df[\"c_usuario_bloqueo\"].fillna(\"\").astype(str).str[:10]\n",
    "    df[\"m_procesado\"]           = df[\"m_procesado\"].fillna(\"N\").astype(str).str[:1]\n",
    "    df[\"c_compra_kikker\"]       = df[\"c_compra_kikker\"].fillna(\"\").astype(str).str[:20]\n",
    "    df[\"c_usuario_modif\"]       = df[\"c_usuario_modif\"].fillna(\"\").astype(str).str[:20]\n",
    "\n",
    "    # Números exactos\n",
    "    df[\"u_prefijo_oc\"] = pd.to_numeric(df[\"u_prefijo_oc\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df[\"u_sufijo_oc\"]  = pd.to_numeric(df[\"u_sufijo_oc\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df[\"c_comprador\"]  = pd.to_numeric(df[\"c_comprador\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Timestamps (permitimos NaT)\n",
    "    df[\"f_genero_oc\"] = df[\"f_genero_oc\"].fillna(pd.Timestamp('1900-01-01 00:00:00.000'))\n",
    "    df[\"f_procesado\"] = df[\"f_procesado\"].fillna(pd.Timestamp('1900-01-01 00:00:00.000'))\n",
    "    #df[\"f_genero_oc\"] = pd.to_datetime(df[\"f_genero_oc\"], errors='coerce')\n",
    "    #df[\"f_procesado\"] = pd.to_datetime(df[\"f_procesado\"], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def validar_longitudes(df):\n",
    "    campos_texto = [\n",
    "        \"c_usuario_genero_oc\", \"c_terminal_genero_oc\", \"c_usuario_bloqueo\",\n",
    "        \"m_procesado\", \"c_compra_kikker\", \"c_usuario_modif\"\n",
    "    ]\n",
    "    print(\"\\n [INFO] Validando longitudes máximas por columna de texto:\")\n",
    "    for col in campos_texto:\n",
    "        max_len = df[col].astype(str).map(len).max()\n",
    "        print(f\"{col}: longitud máxima = {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64138c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_19464\\2353080233.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_oc = pd.read_sql(query, conn_pg) # type: ignore\n"
     ]
    }
   ],
   "source": [
    " # 1. Conexión a PostgreSQL\n",
    "conn_pg = Open_Diarco_Data()\n",
    "if conn_pg is None:\n",
    "    raise ConnectionError(\"[ERROR] No se pudo conectar a PostgreSQL\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM public.t080_oc_precarga_kikker\n",
    "WHERE m_publicado = false\n",
    "\"\"\"\n",
    "df_oc = pd.read_sql(query, conn_pg) # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "# if df_oc.empty:\n",
    "#     logging.warning(\"[WARNING] No hay registros pendientes de publicación\")\n",
    "    #return\n",
    "\n",
    "lista_proveedores = df_oc['c_proveedor'].dropna().astype(int).unique().tolist()\n",
    "\n",
    "# Formatea la lista para usarla en la consulta SQL\n",
    "in_clause = ', '.join([f\"'{prov}'\" for prov in lista_proveedores])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae62ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_33708\\3130845347.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_prod = pd.read_sql(queryp, conn_pg) # type: ignore\n"
     ]
    }
   ],
   "source": [
    " # 1B. Traer productos vigentes de PostgreSQL\n",
    "queryp = f\"\"\"\n",
    "SELECT c_sucu_empr, c_articulo, c_proveedor_primario, abastecimiento, cod_cd\n",
    "FROM src.base_productos_vigentes\n",
    "WHERE c_proveedor_primario IN ({in_clause})\n",
    "\"\"\"\n",
    "\n",
    "df_prod = pd.read_sql(queryp, conn_pg) # type: ignore\n",
    "# if df_prod.empty:\n",
    "#     logging.warning(\"[WARNING] No hay productos relacionados\")\n",
    "#     return\n",
    "\n",
    "# Consolidar artículos y proveedores con abastecimiento = 0\n",
    "# Hacemos el merge con clave múltiple\n",
    "\n",
    "df_merged = df_oc.merge(\n",
    "    df_prod,\n",
    "    how='left',\n",
    "    left_on=['c_sucu_empr', 'c_articulo', 'c_proveedor'],\n",
    "    right_on=['c_sucu_empr', 'c_articulo', 'c_proveedor_primario']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc7d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar por cod_cd = '41CD'\n",
    "df_41= df_merged[df_merged['cod_cd'] == '41CD']\n",
    "if df_41.empty:\n",
    "    logging.warning(\"[WARNING] No hay registros de cod_cd '41CD' para publicar\")\n",
    "else:\n",
    "    df_grouped_41 = df_41.groupby(\n",
    "    ['c_proveedor', 'c_articulo'],\n",
    "    as_index=False\n",
    "        ).agg({\n",
    "            'q_bultos_kilos_diarco': 'sum',\n",
    "            'f_alta_sist': 'first',\n",
    "            'c_usuario_genero_oc': 'first',\n",
    "            'c_terminal_genero_oc': 'first',    \n",
    "            'f_genero_oc': 'first',\n",
    "            'c_usuario_bloqueo': 'first',\n",
    "            'm_procesado': 'first',\n",
    "            'f_procesado': 'first',\n",
    "            'u_prefijo_oc': 'first',\n",
    "            'u_sufijo_oc': 'first',\n",
    "            'c_compra_kikker': 'first',\n",
    "            'c_usuario_modif': 'first',\n",
    "            'c_comprador': 'first'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "    df_grouped_41['c_sucu_empr'] = 41\n",
    "    # Borrar en Origen\n",
    "    df_merged.drop(df_merged[df_merged['cod_cd'] == '41CD'].index, inplace=True)\n",
    "    # Publicar en Destino\n",
    "    df_merged = pd.concat([df_merged, df_grouped_41], ignore_index=True)\n",
    "\n",
    "# Filtrar por cod_cd = '82CD'\n",
    "df_82= df_merged[df_merged['cod_cd'] == '82CD']\n",
    "if df_82.empty:\n",
    "    logging.warning(\"[WARNING] No hay registros de cod_cd '82CD' para publicar\")\n",
    "else:\n",
    "    df_grouped_82 = df_82.groupby(\n",
    "    ['c_proveedor', 'c_articulo'],\n",
    "    as_index=False\n",
    "        ).agg({\n",
    "            'q_bultos_kilos_diarco': 'sum',\n",
    "            'f_alta_sist': 'first',\n",
    "            'c_usuario_genero_oc': 'first',\n",
    "            'c_terminal_genero_oc': 'first',    \n",
    "            'f_genero_oc': 'first',\n",
    "            'c_usuario_bloqueo': 'first',\n",
    "            'm_procesado': 'first',\n",
    "            'f_procesado': 'first',\n",
    "            'u_prefijo_oc': 'first',\n",
    "            'u_sufijo_oc': 'first',\n",
    "            'c_compra_kikker': 'first',\n",
    "            'c_usuario_modif': 'first',\n",
    "            'c_comprador': 'first'\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "    df_grouped_82['c_sucu_empr'] = 82\n",
    "    # Borrar en Origen\n",
    "    df_merged.drop(df_merged[df_merged['cod_cd'] == '82CD'].index, inplace=True)\n",
    "    # Publicar en Destino\n",
    "    df_merged = pd.concat([df_merged, df_grouped_82], ignore_index=True)\n",
    "\n",
    "\n",
    "# Agrupar por proveedor y artículo, sumando cantidades y tomando el primer valor del resto\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f6dfcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
       "       'f_alta_sist', 'c_usuario_genero_oc', 'c_terminal_genero_oc',\n",
       "       'f_genero_oc', 'c_usuario_bloqueo', 'm_procesado', 'f_procesado',\n",
       "       'u_prefijo_oc', 'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif',\n",
       "       'c_comprador', 'm_publicado'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0122e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
       "       'f_alta_sist', 'c_usuario_genero_oc', 'c_terminal_genero_oc',\n",
       "       'f_genero_oc', 'c_usuario_bloqueo', 'm_procesado', 'f_procesado',\n",
       "       'u_prefijo_oc', 'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif',\n",
       "       'c_comprador', 'm_publicado', 'c_proveedor_primario', 'abastecimiento',\n",
       "       'cod_cd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a256078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar por cod_cd = '41CD'\n",
    "df_directo = df_merged[df_merged['cod_cd'] != '41CD']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301237",
   "metadata": {},
   "source": [
    "## PRUEBA BLOQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2cf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidar_oc_precarga():\n",
    "    logging.info(\"[INFO] Iniciando consolidación por abastecimiento\")  \n",
    "    conn_pg = None\n",
    "\n",
    "    try:\n",
    "         # 1. Conexión a PostgreSQL\n",
    "        conn_pg = Open_Diarco_Data()\n",
    "        if conn_pg is None:\n",
    "            raise ConnectionError(\"[ERROR] No se pudo conectar a PostgreSQL\")\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM public.t080_oc_precarga_kikker\n",
    "        WHERE m_publicado = false\n",
    "        \"\"\"\n",
    "        df_oc = pd.read_sql(query, conn_pg) # type: ignore\n",
    "\n",
    "        if df_oc.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros pendientes de publicación\")\n",
    "            return\n",
    "\n",
    "        # Convertir a enteros antes de armar la cláusula IN\n",
    "        lista_proveedores = (\n",
    "            pd.to_numeric(df_oc['c_proveedor'], errors='coerce')\n",
    "            .dropna()\n",
    "            .astype(int)\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "\n",
    "        # Formatea la lista para usarla en la consulta SQL\n",
    "        in_clause = ', '.join([f\"'{prov}'\" for prov in lista_proveedores])\n",
    "\n",
    "        # 1B. Traer productos vigentes de PostgreSQL\n",
    "        queryp = f\"\"\"\n",
    "        SELECT c_sucu_empr, c_articulo, c_proveedor_primario, abastecimiento, cod_cd\n",
    "        FROM src.base_productos_vigentes\n",
    "        WHERE c_proveedor_primario IN ({in_clause})\n",
    "        \"\"\"\n",
    "        df_prod = pd.read_sql(queryp, conn_pg) # type: ignore\n",
    "\n",
    "        df_merged = df_oc.merge(\n",
    "            df_prod,\n",
    "            how='left',\n",
    "            left_on=['c_sucu_empr', 'c_articulo', 'c_proveedor'],\n",
    "            right_on=['c_sucu_empr', 'c_articulo', 'c_proveedor_primario']\n",
    "        )\n",
    "\n",
    "        # Filtrar por cod_cd = '41CD'\n",
    "        df_41= df_merged[df_merged['cod_cd'] == '41CD']\n",
    "        if df_41.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros de cod_cd '41CD' para publicar\")\n",
    "        else:\n",
    "            df_grouped_41 = df_41.groupby(\n",
    "            ['c_proveedor', 'c_articulo'],\n",
    "            as_index=False\n",
    "                ).agg({\n",
    "                    'q_bultos_kilos_diarco': 'sum',\n",
    "                    'f_alta_sist': 'first',\n",
    "                    'c_usuario_genero_oc': 'first',\n",
    "                    'c_terminal_genero_oc': 'first',    \n",
    "                    'f_genero_oc': 'first',\n",
    "                    'c_usuario_bloqueo': 'first',\n",
    "                    'm_procesado': 'first',\n",
    "                    'f_procesado': 'first',\n",
    "                    'u_prefijo_oc': 'first',\n",
    "                    'u_sufijo_oc': 'first',\n",
    "                    'c_compra_kikker': 'first',\n",
    "                    'c_usuario_modif': 'first',\n",
    "                    'c_comprador': 'first'\n",
    "                }).reset_index(drop=True)\n",
    "\n",
    "            df_grouped_41['c_sucu_empr'] = 41\n",
    "            # Borrar en Origen\n",
    "            df_merged.drop(df_merged[df_merged['cod_cd'] == '41CD'].index, inplace=True)\n",
    "            # Publicar en Destino\n",
    "            df_merged = pd.concat([df_merged, df_grouped_41], ignore_index=True)\n",
    "\n",
    "        # Filtrar por cod_cd = '82CD'\n",
    "        df_82= df_merged[df_merged['cod_cd'] == '82CD']\n",
    "        if df_82.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros de cod_cd '82CD' para publicar\")\n",
    "        else:\n",
    "            df_grouped_82 = df_82.groupby(\n",
    "            ['c_proveedor', 'c_articulo'],\n",
    "            as_index=False\n",
    "                ).agg({\n",
    "                    'q_bultos_kilos_diarco': 'sum',\n",
    "                    'f_alta_sist': 'first',\n",
    "                    'c_usuario_genero_oc': 'first',\n",
    "                    'c_terminal_genero_oc': 'first',    \n",
    "                    'f_genero_oc': 'first',\n",
    "                    'c_usuario_bloqueo': 'first',\n",
    "                    'm_procesado': 'first',\n",
    "                    'f_procesado': 'first',\n",
    "                    'u_prefijo_oc': 'first',\n",
    "                    'u_sufijo_oc': 'first',\n",
    "                    'c_compra_kikker': 'first',\n",
    "                    'c_usuario_modif': 'first',\n",
    "                    'c_comprador': 'first'\n",
    "                }).reset_index(drop=True)\n",
    "\n",
    "            df_grouped_82['c_sucu_empr'] = 82\n",
    "            # Borrar en Origen\n",
    "            df_merged.drop(df_merged[df_merged['cod_cd'] == '82CD'].index, inplace=True)\n",
    "            # Publicar en Destino\n",
    "            df_merged = pd.concat([df_merged, df_grouped_82], ignore_index=True)\n",
    "\n",
    "        \n",
    "        # 1C. Traer Stock CENTROS DE DISTRIBUCIÓN\n",
    "        querystock = f\"\"\"\n",
    "         SELECT S.c_sucu_empr ,S.c_articulo ,S.q_peso_articulo ,P.q_factor_proveedor\n",
    "\t            ,S.q_unid_articulo / p.q_factor_proveedor as stock\n",
    "            FROM src.t060_stock S\n",
    "            LEFT JOIN src.t052_articulos_proveedor P\n",
    "                ON S.c_articulo = P.c_articulo\n",
    "                WHERE P.c_proveedor in ({in_clause}) \n",
    "                and S.c_sucu_empr IN(41, 82)\n",
    "        \"\"\"\n",
    "\n",
    "        df_stock = pd.read_sql(querystock, conn_pg) # type: ignore\n",
    "        if df_stock.empty:\n",
    "            logging.warning(\"[WARNING] No hay stock disponible para los proveedores seleccionados\")\n",
    "        else:\n",
    "            df_stock.rename(columns={'c_sucu_empr': 'c_sucu_empr_stock', 'c_articulo': 'c_articulo_stock'}, inplace=True)\n",
    "            # Hacemos el merge con clave múltiple\n",
    "            # Esto agrega el stock a df_merged\n",
    "            df_merged = df_merged.merge(\n",
    "                df_stock[['c_sucu_empr_stock', 'c_articulo_stock', 'stock']],\n",
    "                how='left',\n",
    "                left_on=['c_sucu_empr', 'c_articulo'],\n",
    "                right_on=['c_sucu_empr_stock', 'c_articulo_stock']\n",
    "            )\n",
    "            # Restar a q_bultos_kilos_diarco stock y tranformar a entero\n",
    "            df_merged['q_bultos_kilos_diarco'] = (\n",
    "                df_merged['q_bultos_kilos_diarco'] - df_merged['stock']\n",
    "                ).clip(lower=0).astype(int) \n",
    "            # Eliminar columnas de stock\n",
    "            df_merged.drop(columns=['c_sucu_empr_stock', 'c_articulo_stock', 'stock'], inplace=True)\n",
    "\n",
    "        conn_pg.close()\n",
    "        return df_merged \n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"[ERROR] Error durante la CONSOLIDACIÓN de OC Precarga\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(\"[ERROR] Error durante la ejecución:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e32f1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def publicar_oc_precarga():\n",
    "    logging.info(\"[INFO] Iniciando publicación de OC Precarga\")\n",
    "    df_oc = consolidar_oc_precarga()\n",
    "    if df_oc is None or df_oc.empty:\n",
    "        logging.warning(\"[WARNING] No hay registros consolidados para publicar\")\n",
    "        return\n",
    "\n",
    "    # Abrir conexión a PostgreSQL SOLO para el update\n",
    "    conn_pg = Open_Diarco_Data()\n",
    "    if conn_pg is None:\n",
    "        raise ConnectionError(\"[ERROR] No se pudo reconectar a PostgreSQL para actualizar publicados\")\n",
    "\n",
    "    conn_sql = None\n",
    "    cursor_sql = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if df_oc.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros pendientes de publicación\")\n",
    "            return\n",
    "\n",
    "        # Agrupar por c_proveedor_primario y c_articulo, sumando las cantidades\n",
    "        total_rows = len(df_oc)\n",
    "        logging.info(f\"[INFO] Registros a publicar: {total_rows}\")\n",
    "\n",
    "        df_oc = limpiar_campos_oc(df_oc)\n",
    "        validar_longitudes(df_oc)\n",
    "        print(df_oc.head(5))\n",
    "\n",
    "        # 2. Conexión a SQL Server\n",
    "        conn_sql = Open_Connection()\n",
    "        if conn_sql is None:\n",
    "            raise ConnectionError(\"[ERROR] No se pudo conectar a SQL Server\")\n",
    "\n",
    "        cursor_sql = conn_sql.cursor()\n",
    "        cursor_sql.fast_executemany = True  # validar si es soportado por tu driver\n",
    "\n",
    "        insert_stmt = \"\"\"\n",
    "        INSERT INTO [dbo].[T080_OC_PRECARGA_KIKKER] (\n",
    "            [C_PROVEEDOR], [C_ARTICULO], [C_SUCU_EMPR], [Q_BULTOS_KILOS_DIARCO],\n",
    "            [F_ALTA_SIST], [C_USUARIO_GENERO_OC], [C_TERMINAL_GENERO_OC], [F_GENERO_OC],\n",
    "            [C_USUARIO_BLOQUEO], [M_PROCESADO], [F_PROCESADO], [U_PREFIJO_OC],\n",
    "            [U_SUFIJO_OC], [C_COMPRA_KIKKER], [C_USUARIO_MODIF], [C_COMPRADOR]\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "\n",
    "        data_tuples = df_oc[[\n",
    "            'c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
    "            'f_alta_sist', 'c_usuario_genero_oc', 'c_terminal_genero_oc', 'f_genero_oc',\n",
    "            'c_usuario_bloqueo', 'm_procesado', 'f_procesado', 'u_prefijo_oc',\n",
    "            'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif', 'c_comprador'\n",
    "        ]].itertuples(index=False, name=None)\n",
    "\n",
    "        cursor_sql.executemany(insert_stmt, list(data_tuples))\n",
    "        conn_sql.commit()\n",
    "\n",
    "        logging.info(\"[INFO] Inserción completada en SQL Server\")\n",
    "        print(f\"✔ Se insertaron {total_rows} registros en SQL Server.\")\n",
    "\n",
    "        # 3. Marcar como publicados en PostgreSQL\n",
    "        lista_compra_kikker = df_oc['c_compra_kikker'].dropna().unique().tolist()\n",
    "        placeholders = ', '.join(['%s'] * len(lista_compra_kikker))\n",
    "        update_stmt = f\"\"\"\n",
    "                UPDATE public.t080_oc_precarga_kikker\n",
    "                SET m_publicado = true\n",
    "                WHERE c_compra_kikker IN ({placeholders})\n",
    "            \"\"\"\n",
    "\n",
    "        with conn_pg.cursor() as cursor_pg:\n",
    "            cursor_pg.execute(update_stmt, lista_compra_kikker)\n",
    "            rows_updated = cursor_pg.rowcount\n",
    "            conn_pg.commit()\n",
    "\n",
    "        logging.info(f\"[INFO] {rows_updated} registros marcados como publicados\")\n",
    "        print(f\"✔ {rows_updated} registros actualizados con m_publicado = true\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"[ERROR] Error durante la publicación de OC Precarga\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(\"[ERROR] Error durante la ejecución:\", e)\n",
    "\n",
    "    finally:\n",
    "        if cursor_sql:\n",
    "            try:\n",
    "                cursor_sql.close()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[WARNING] Error al cerrar cursor SQL: {e}\")\n",
    "        if conn_sql:\n",
    "            try:\n",
    "                conn_sql.close()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[WARNING] Error al cerrar conexión SQL Server: {e}\")\n",
    "        if conn_pg:\n",
    "            try:\n",
    "                conn_pg.close()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[WARNING] Error al cerrar conexión PostgreSQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822cdc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Proceso finalizado. Ver log en: ./pruebas/logs\\publicacion_oc_precarga.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_19464\\2326433591.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_oc = pd.read_sql(query, conn_pg) # type: ignore\n"
     ]
    }
   ],
   "source": [
    "publicar_oc_precarga()\n",
    "print(f\"[INFO] Proceso finalizado. Ver log en: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
