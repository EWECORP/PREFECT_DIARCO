{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee8b951",
   "metadata": {},
   "source": [
    "## Pruebas de Consolidación de OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea680a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import psycopg2 as pg2\n",
    "\n",
    "# Cargar configuración DINAMICA de acuerdo al entorno\n",
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "import io\n",
    "\n",
    "ENV_PATH = os.environ.get(\"ETL_ENV_PATH\", \"E:/ETL/ETL_DIARCO/.env\")  # Toma Producción si está definido, o la ruta por defecto E:\\ETL\\ETL_DIARCO\\.env\n",
    "# Verificar si el archivo .env existe\n",
    "if not os.path.exists(ENV_PATH):\n",
    "    print(f\"El archivo .env no existe en la ruta: {ENV_PATH}\")\n",
    "    print(f\"Directorio actual: {os.getcwd()}\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "secrets = dotenv_values(ENV_PATH)\n",
    "folder = f\"E:/ETL/ETL_DIARCO/{secrets['FOLDER_DATOS']}\"\n",
    "folder_logs = f\"E:/ETL/ETL_DIARCO/{secrets['FOLDER_LOG']}\"\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73f389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARANDO LOGS : Directorio actual: e:\\ETL\\ETL_DIARCO\\scripts\\pull\n",
      "[INFO] Cargando configuración desde: E:/ETL/ETL_DIARCO/.env\n",
      "[INFO] Carpeta de datos: E:/ETL/ETL_DIARCO/data\n"
     ]
    }
   ],
   "source": [
    "# Funciones Locales\n",
    "def Open_Connection():\n",
    "    conn_str = f'DRIVER={secrets[\"SQLP_DRIVER\"]};SERVER={secrets[\"SQLP_SERVER\"]};PORT={secrets[\"SQLP_PORT\"]};DATABASE={secrets[\"SQLP_DATABASE\"]};UID={secrets[\"SQLP_USER\"]};PWD={secrets[\"SQLP_PASSWORD\"]}'\n",
    "    # print (conn_str) \n",
    "    try:    \n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        return conn\n",
    "    except:\n",
    "        print('Error en la Conexión')\n",
    "        return None\n",
    "\n",
    "def Open_Diarco_Data(): \n",
    "    conn_str = f\"dbname={secrets['PG_DB']} user={secrets['PG_USER']} password={secrets['PG_PASSWORD']} host={secrets['PG_HOST']} port={secrets['PG_PORT']}\"\n",
    "    #print (conn_str)\n",
    "    for i in range(5):\n",
    "        try:    \n",
    "            conn = pg2.connect(conn_str)\n",
    "            return conn\n",
    "        except Exception as e:\n",
    "            print(f'Error en la conexión: {e}')\n",
    "            time.sleep(5)\n",
    "    return None  # Retorna None si todos los intentos fallan\n",
    "\n",
    "def Close_Connection(conn): \n",
    "    if conn is not None:\n",
    "        conn.close()\n",
    "        # print(\"[OK] Conexión cerrada.\")    \n",
    "    return True\n",
    "\n",
    "# -----------------------------------\n",
    "print(f\"PREPARANDO LOGS : Directorio actual: {os.getcwd()}\")\n",
    "print(f\"[INFO] Cargando configuración desde: {ENV_PATH}\")\n",
    "print(f\"[INFO] Carpeta de datos: {folder}\") \n",
    "os.makedirs(folder_logs, exist_ok=True)\n",
    "\n",
    "log_file = os.path.join(folder_logs, \"publicacion_oc_precarga.log\")\n",
    "\n",
    "#\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "\n",
    "def limpiar_campos_oc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # --- Texto con longitudes destino ---\n",
    "    df[\"c_usuario_genero_oc\"]  = df[\"c_usuario_genero_oc\"].fillna(\"\").astype(str).str[:10]\n",
    "    df[\"c_terminal_genero_oc\"] = df[\"c_terminal_genero_oc\"].fillna(\"\").astype(str).str[:15]\n",
    "    df[\"c_usuario_bloqueo\"]    = df[\"c_usuario_bloqueo\"].fillna(\"\").astype(str).str[:10]\n",
    "    df[\"m_procesado\"]          = df[\"m_procesado\"].fillna(\"N\").astype(str).str[:1]\n",
    "    df[\"c_compra_kikker\"]      = df[\"c_compra_kikker\"].fillna(\"\").astype(str).str[:20]\n",
    "    df[\"c_usuario_modif\"]      = df[\"c_usuario_modif\"].fillna(\"\").astype(str).str[:20]\n",
    "\n",
    "    # --- Claves y numéricos EXACTOS como INT ---\n",
    "    for col in [\"c_proveedor\", \"c_articulo\", \"c_sucu_empr\", \"u_prefijo_oc\", \"u_sufijo_oc\", \"c_comprador\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Cantidad a publicar en bultos/kilos como INT (>=0)\n",
    "    if \"q_bultos_kilos_diarco\" in df.columns:\n",
    "        df[\"q_bultos_kilos_diarco\"] = pd.to_numeric(df[\"q_bultos_kilos_diarco\"], errors=\"coerce\").fillna(0)\n",
    "        df[\"q_bultos_kilos_diarco\"] = df[\"q_bultos_kilos_diarco\"].clip(lower=0).astype(int)\n",
    "\n",
    "    # --- Timestamps ---\n",
    "    for dcol in [\"f_alta_sist\", \"f_genero_oc\", \"f_procesado\"]:\n",
    "        if dcol in df.columns:\n",
    "            df[dcol] = pd.to_datetime(df.get(dcol), errors='coerce') # type: ignore\n",
    "    if \"f_genero_oc\" in df.columns:\n",
    "        df[\"f_genero_oc\"] = df[\"f_genero_oc\"].fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "    if \"f_procesado\" in df.columns:\n",
    "        df[\"f_procesado\"] = df[\"f_procesado\"].fillna(pd.Timestamp('1900-01-01 00:00:00'))\n",
    "\n",
    "    # --- Deduplicar intratable por PK destino ---\n",
    "    pk_cols = [\"c_proveedor\", \"c_articulo\", \"c_sucu_empr\"]\n",
    "    pk_cols = [c for c in pk_cols if c in df.columns]\n",
    "    if pk_cols:\n",
    "        df = df.drop_duplicates(subset=pk_cols, keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def forzar_enteros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Columnas clave y numéricos exactos\n",
    "    int_cols = [\n",
    "        \"c_proveedor\", \"c_articulo\", \"c_sucu_empr\",\n",
    "        \"u_prefijo_oc\", \"u_sufijo_oc\", \"c_comprador\",\n",
    "        \"q_bultos_kilos_diarco\", \"c_proveedor_primario\",\n",
    "        \"abastecimiento\"\n",
    "    ]\n",
    "    for col in int_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "    # Booleanos\n",
    "    if \"m_publicado\" in df.columns:\n",
    "        df[\"m_publicado\"] = df[\"m_publicado\"].fillna(False).astype(bool)\n",
    "\n",
    "    # Fechas (por si se degradaron)\n",
    "    for dcol in [\"f_alta_sist\", \"f_genero_oc\", \"f_procesado\"]:\n",
    "        if dcol in df.columns:\n",
    "            df[dcol] = pd.to_datetime(df[dcol], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def validar_longitudes(df):\n",
    "    campos_texto = [\n",
    "        \"c_usuario_genero_oc\", \"c_terminal_genero_oc\", \"c_usuario_bloqueo\",\n",
    "        \"m_procesado\", \"c_compra_kikker\", \"c_usuario_modif\"\n",
    "    ]\n",
    "    print(\"\\n [INFO] Validando longitudes máximas por columna de texto:\")\n",
    "    for col in campos_texto:\n",
    "        max_len = df[col].astype(str).map(len).max()\n",
    "        print(f\"{col}: longitud máxima = {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64138c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_33292\\1451225749.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_oc = pd.read_sql(query, conn_pg) # type: ignore\n"
     ]
    }
   ],
   "source": [
    " # 1. Conexión a PostgreSQL\n",
    "conn_pg = Open_Diarco_Data()\n",
    "if conn_pg is None:\n",
    "    raise ConnectionError(\"[ERROR] No se pudo conectar a PostgreSQL\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM public.t080_oc_precarga_kikker\n",
    "WHERE m_publicado = false\n",
    "AND c_proveedor = 11740\n",
    "\"\"\"\n",
    "df_oc = pd.read_sql(query, conn_pg) # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "# if df_oc.empty:\n",
    "#     logging.warning(\"[WARNING] No hay registros pendientes de publicación\")\n",
    "    #return\n",
    "\n",
    "lista_proveedores = df_oc['c_proveedor'].dropna().astype(int).unique().tolist()\n",
    "\n",
    "# Formatea la lista para usarla en la consulta SQL\n",
    "in_clause = ', '.join([f\"'{prov}'\" for prov in lista_proveedores])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae62ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eduar\\AppData\\Local\\Temp\\ipykernel_33292\\4056776813.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_prod = pd.read_sql(queryp, conn_pg) # type: ignore\n"
     ]
    }
   ],
   "source": [
    "# 1B. Traer productos vigentes de PostgreSQL\n",
    "queryp = f\"\"\"\n",
    "SELECT c_sucu_empr, c_articulo, c_proveedor_primario, abastecimiento, cod_cd\n",
    "FROM src.base_productos_vigentes\n",
    "WHERE c_proveedor_primario IN ({in_clause})\n",
    "\"\"\"\n",
    "\n",
    "df_prod = pd.read_sql(queryp, conn_pg) # type: ignore\n",
    "# if df_prod.empty:\n",
    "#     logging.warning(\"[WARNING] No hay productos relacionados\")\n",
    "#     return\n",
    "\n",
    "df_prod = forzar_enteros(df_prod)\n",
    "\n",
    "# Merge completo\n",
    "df_completo = df_oc.merge(\n",
    "    df_prod,\n",
    "    how='left',\n",
    "    left_on=['c_sucu_empr', 'c_articulo', 'c_proveedor'],\n",
    "    right_on=['c_sucu_empr', 'c_articulo', 'c_proveedor_primario']\n",
    ")\n",
    "df_completo = forzar_enteros(df_completo)\n",
    "if 'c_proveedor_primario' in df_completo.columns:\n",
    "    df_completo.drop(columns=['c_proveedor_primario'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc7d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# PARTES A PUBLICAR\n",
    "# -------------------------------\n",
    "partes = []\n",
    "\n",
    "# 41CD (consolidado a sucursal 41)\n",
    "df_41 = df_completo[df_completo['cod_cd'] == '41CD']\n",
    "if not df_41.empty:\n",
    "    df_grouped_41 = df_41.groupby(['c_proveedor', 'c_articulo'], as_index=False).agg({\n",
    "        'q_bultos_kilos_diarco': 'sum',\n",
    "        'f_alta_sist': 'first',\n",
    "        'c_usuario_genero_oc': 'first',\n",
    "        'c_terminal_genero_oc': 'first',\n",
    "        'f_genero_oc': 'first',\n",
    "        'c_usuario_bloqueo': 'first',\n",
    "        'm_procesado': 'first',\n",
    "        'f_procesado': 'first',\n",
    "        'u_prefijo_oc': 'first',\n",
    "        'u_sufijo_oc': 'first',\n",
    "        'c_compra_kikker': 'first',\n",
    "        'c_usuario_modif': 'first',\n",
    "        'c_comprador': 'first'\n",
    "    }).reset_index(drop=True)\n",
    "    df_grouped_41['c_sucu_empr'] = 41\n",
    "    df_grouped_41['m_publicado'] = False\n",
    "    partes.append(df_grouped_41)\n",
    "    logging.info(f\"[INFO] Registros consolidados para 41CD: {len(df_grouped_41)}\")\n",
    "else:\n",
    "    logging.warning(\"[WARNING] No hay registros de cod_cd '41CD' para publicar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9644adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 82CD (consolidado a sucursal 82)\n",
    "df_82 = df_completo[df_completo['cod_cd'] == '82CD']\n",
    "if not df_82.empty:\n",
    "    df_grouped_82 = df_82.groupby(['c_proveedor', 'c_articulo'], as_index=False).agg({\n",
    "        'q_bultos_kilos_diarco': 'sum',\n",
    "        'f_alta_sist': 'first',\n",
    "        'c_usuario_genero_oc': 'first',\n",
    "        'c_terminal_genero_oc': 'first',\n",
    "        'f_genero_oc': 'first',\n",
    "        'c_usuario_bloqueo': 'first',\n",
    "        'm_procesado': 'first',\n",
    "        'f_procesado': 'first',\n",
    "        'u_prefijo_oc': 'first',\n",
    "        'u_sufijo_oc': 'first',\n",
    "        'c_compra_kikker': 'first',\n",
    "        'c_usuario_modif': 'first',\n",
    "        'c_comprador': 'first'\n",
    "    }).reset_index(drop=True)\n",
    "    df_grouped_82['c_sucu_empr'] = 82\n",
    "    df_grouped_82['m_publicado'] = False\n",
    "    partes.append(df_grouped_82)\n",
    "    logging.info(f\"[INFO] Registros consolidados para 82CD: {len(df_grouped_82)}\")\n",
    "else:\n",
    "    logging.warning(\"[WARNING] No hay registros de cod_cd '82CD' para publicar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263021bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSTHROUGH (entregas directas: no consolidan)\n",
    "# Tomamos todo lo que NO sea 41CD/82CD (incluye NULL en cod_cd) y lo dejamos tal cual.\n",
    "mask_passthrough = (~df_completo['cod_cd'].isin(['41CD', '82CD'])) | (df_completo['cod_cd'].isna())\n",
    "df_passthrough = df_completo.loc[mask_passthrough].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ea8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Registros de entrega directa (passthrough): 45\n"
     ]
    }
   ],
   "source": [
    "if not df_passthrough.empty:\n",
    "    # Homogeneizar columnas a las de las partes consolidadas\n",
    "    cols_out = [\n",
    "        'c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
    "        'f_alta_sist', 'c_usuario_genero_oc', 'c_terminal_genero_oc', 'f_genero_oc',\n",
    "        'c_usuario_bloqueo', 'm_procesado', 'f_procesado', 'u_prefijo_oc',\n",
    "        'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif', 'c_comprador'\n",
    "    ]\n",
    "    # Asegurar existencia de columnas\n",
    "    for c in cols_out:\n",
    "        if c not in df_passthrough.columns:\n",
    "            df_passthrough[c] = pd.NA\n",
    "\n",
    "    df_passthrough = df_passthrough[cols_out].reset_index(drop=True)\n",
    "    df_passthrough['m_publicado'] = False\n",
    "    df_passthrough = forzar_enteros(df_passthrough)\n",
    "    partes.append(df_passthrough)\n",
    "    print(f\"[INFO] Registros de entrega directa (passthrough): {len(df_passthrough)}\")\n",
    "else:\n",
    "    print(\"[INFO] No hay registros de entrega directa (passthrough) para publicar\")\n",
    "\n",
    "# Si no hay nada que publicar, salir\n",
    "if not partes:\n",
    "    print(\"[WARNING] No hay registros con cod_cd {41CD,82CD} ni entregas directas para publicar\")\n",
    "    conn_pg.close()\n",
    "    #return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158a7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# CONCATENACIÓN EXPLÍCITA (grouped + passthrough)\n",
    "# -----------------------------------\n",
    "frames = []\n",
    "\n",
    "# Aseguramos variables y tamaños (0 si no existen o están vacías)\n",
    "n_g41 = len(df_grouped_41) if 'df_grouped_41' in locals() and isinstance(df_grouped_41, pd.DataFrame) and not df_grouped_41.empty else 0\n",
    "n_g82 = len(df_grouped_82) if 'df_grouped_82' in locals() and isinstance(df_grouped_82, pd.DataFrame) and not df_grouped_82.empty else 0\n",
    "n_dir = len(df_passthrough) if 'df_passthrough' in locals() and isinstance(df_passthrough, pd.DataFrame) and not df_passthrough.empty else 0\n",
    "\n",
    "if n_g41 > 0:\n",
    "    frames.append(df_grouped_41)\n",
    "if n_g82 > 0:\n",
    "    frames.append(df_grouped_82)\n",
    "if n_dir > 0:\n",
    "    frames.append(df_passthrough)\n",
    "\n",
    "if not frames:\n",
    "    logging.warning(\"[WARNING] No hay partes para concatenar (41CD agrupado, 82CD agrupado ni directos)\")\n",
    "    conn_pg.close()\n",
    "   # return pd.DataFrame()\n",
    "\n",
    "# Concatenar uno debajo del otro en orden: 41 → 82 → directos\n",
    "df_merged = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "logging.info(\n",
    "    f\"[INFO] Total registros concatenados: {len(df_merged)} \"\n",
    "    f\"(41CD_agrupados={n_g41}, 82CD_agrupados={n_g82}, Directos={n_dir})\"\n",
    ")\n",
    "\n",
    "\n",
    "# Definir el orden deseado de columnas\n",
    "orden_columnas = [\n",
    "    'c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
    "    'f_alta_sist', 'c_usuario_genero_oc', 'c_terminal_genero_oc', 'f_genero_oc',\n",
    "    'c_usuario_bloqueo', 'm_procesado', 'f_procesado',\n",
    "    'u_prefijo_oc', 'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif',\n",
    "    'c_comprador'\n",
    "]\n",
    "# Reordenar el DataFrame\n",
    "df_merged = df_merged[orden_columnas]\n",
    "\n",
    "\n",
    "# Normalización final de tipos enteros/fechas/booleanos\n",
    "df_merged = forzar_enteros(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5aea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Agrupar y sumar la cantidad\n",
    "df_sumado = df_merged.groupby(['c_proveedor', 'c_articulo', 'c_sucu_empr'], as_index=False)['q_bultos_kilos_diarco'].sum()\n",
    "\n",
    "# Paso 2: Eliminar duplicados del original (conservando la primera ocurrencia)\n",
    "df_sin_duplicados = df_merged.drop_duplicates(subset=['c_proveedor', 'c_articulo', 'c_sucu_empr'], keep='first')\n",
    "\n",
    "# Paso 3: Eliminar la columna original de cantidad para evitar conflicto\n",
    "df_sin_duplicados = df_sin_duplicados.drop(columns=['q_bultos_kilos_diarco'])\n",
    "\n",
    "# Paso 4: Reincorporar la cantidad sumada\n",
    "df_final = df_sin_duplicados.merge(df_sumado, on=['c_proveedor', 'c_articulo', 'c_sucu_empr'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2da2f",
   "metadata": {},
   "source": [
    "## Hasta acá la generación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f862f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1C. Traer Stock CENTROS DE DISTRIBUCIÓN (solo afecta 41/82)\n",
    "querystock = f\"\"\"\n",
    "    SELECT \n",
    "        codigo_sucursal AS c_sucu_empr, \n",
    "        codigo_articulo AS c_articulo,\n",
    "        P.q_factor_compra,\n",
    "        COALESCE(stock, 0) AS stock_origen,\n",
    "        COALESCE(pedido_pendiente, 0) AS pedido_pendiente, \n",
    "        COALESCE(transfer_pendiente, 0) AS transfer_pendiente, \n",
    "        FLOOR(\n",
    "            (COALESCE(stock, 0) + COALESCE(pedido_pendiente, 0) + COALESCE(transfer_pendiente, 0)) \n",
    "            / COALESCE(P.q_factor_compra,1 )\n",
    "        ) AS stock\n",
    "    FROM src.base_stock_sucursal\n",
    "    LEFT JOIN src.base_productos_vigentes P\n",
    "        ON codigo_sucursal = c_sucu_empr \n",
    "        AND codigo_articulo  = c_articulo\n",
    "    WHERE codigo_proveedor IN ({in_clause}) \n",
    "        AND codigo_sucursal IN (41, 82)\n",
    "\"\"\"\n",
    "df_stock = pd.read_sql(querystock, conn_pg)  # type: ignore\n",
    "df_stock = forzar_enteros(df_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8339644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de diagnósticos\n",
    "try:\n",
    "    df_completo.to_csv(os.path.join(folder_logs, f\"{timestamp}_origen_completo.csv\"),\n",
    "                        index=False, encoding='utf-8-sig')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not df_stock.empty:\n",
    "    df_stock.rename(columns={'c_sucu_empr': 'c_sucu_empr_stock', 'c_articulo': 'c_articulo_stock'}, inplace=True)\n",
    "    try:\n",
    "        df_stock.to_csv(os.path.join(folder_logs, f\"{timestamp}_stock.csv\"),\n",
    "                        index=False, encoding='utf-8-sig')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Merge con stock por clave (sucursal, articulo)\n",
    "    df_final = df_final.merge(\n",
    "        df_stock[['c_sucu_empr_stock', 'c_articulo_stock', 'stock']],\n",
    "        how='left',\n",
    "        left_on=['c_sucu_empr', 'c_articulo'],\n",
    "        right_on=['c_sucu_empr_stock', 'c_articulo_stock']\n",
    "    )\n",
    "    df_final.drop(columns=['c_sucu_empr_stock', 'c_articulo_stock'], inplace=True)\n",
    "\n",
    "    # Ajuste de cantidad por stock (solo afectará a 41/82; directas no matchean y quedan sin descuento)\n",
    "    if 'stock' in df_final.columns:\n",
    "        df_final['q_bultos_kilos_diarco'] = (\n",
    "            df_final['q_bultos_kilos_diarco'].fillna(0) - df_final['stock'].fillna(0)\n",
    "        ).clip(lower=0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce175860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de columnas si existen\n",
    "for col in ['abastecimiento', 'cod_cd', 'stock']:\n",
    "    if col in df_final.columns:\n",
    "        df_final.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Filtrar cantidades > 0\n",
    "if 'q_bultos_kilos_diarco' in df_final.columns:\n",
    "    df_final = df_final[df_final['q_bultos_kilos_diarco'] > 0].reset_index(drop=True)\n",
    "\n",
    "# Unicidad intra-lote por PK destino\n",
    "pk_cols = ['c_proveedor', 'c_articulo', 'c_sucu_empr']\n",
    "total = len(df_final)\n",
    "unicas = len(df_final.drop_duplicates(subset=pk_cols))\n",
    "if total != unicas:\n",
    "    logging.warning(f\"[WARN] Duplicados intra-batch detectados: {total - unicas}\")\n",
    "    try:\n",
    "        df_final[df_final.duplicated(subset=pk_cols, keep=False)].to_csv(\n",
    "            os.path.join(folder_logs, f\"{timestamp}_duplicados_intra_batch.csv\"),\n",
    "            index=False, encoding='utf-8-sig'\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    df_final = df_final.drop_duplicates(subset=pk_cols, keep='last').reset_index(drop=True)\n",
    "\n",
    "# Guardar pedido consolidado\n",
    "try:\n",
    "    df_final.to_csv(os.path.join(folder_logs, f\"{timestamp}_pedido_consolidado.csv\"),\n",
    "                        index=False, encoding='utf-8-sig')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el orden deseado de columnas\n",
    "orden_columnas = [\n",
    "    'c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
    "    'f_alta_sist', 'c_usuario_bloqueo', 'm_procesado', 'f_procesado',\n",
    "    'u_prefijo_oc', 'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif',\n",
    "    'c_comprador', 'm_publicado'\n",
    "]\n",
    "# Reordenar el DataFrame\n",
    "df_final = df_final[orden_columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a4242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6dfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0122e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar por cod_cd = '41CD'\n",
    "df_directo = df_merged[df_merged['cod_cd'] != '41CD']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301237",
   "metadata": {},
   "source": [
    "## PRUEBA BLOQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2cf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidar_oc_precarga():\n",
    "    logging.info(\"[INFO] Iniciando consolidación por abastecimiento\")  \n",
    "    conn_pg = None\n",
    "\n",
    "    try:\n",
    "        # 1. Conexión a PostgreSQL\n",
    "        conn_pg = Open_Diarco_Data()\n",
    "        if conn_pg is None:\n",
    "            raise ConnectionError(\"[ERROR] No se pudo conectar a PostgreSQL\")\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM public.t080_oc_precarga_kikker\n",
    "        WHERE m_publicado = false\n",
    "        \"\"\"\n",
    "        df_oc = pd.read_sql(query, conn_pg) # type: ignore\n",
    "\n",
    "        if df_oc.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros pendientes de publicación\")\n",
    "            return\n",
    "\n",
    "        # Convertir a enteros antes de armar la cláusula IN\n",
    "        lista_proveedores = (\n",
    "            pd.to_numeric(df_oc['c_proveedor'], errors='coerce')\n",
    "            .dropna()\n",
    "            .astype(int)\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "\n",
    "        # Formatea la lista para usarla en la consulta SQL\n",
    "        in_clause = ', '.join([f\"'{prov}'\" for prov in lista_proveedores])\n",
    "\n",
    "        # 1B. Traer productos vigentes de PostgreSQL\n",
    "        queryp = f\"\"\"\n",
    "        SELECT c_sucu_empr, c_articulo, c_proveedor_primario, abastecimiento, cod_cd\n",
    "        FROM src.base_productos_vigentes\n",
    "        WHERE c_proveedor_primario IN ({in_clause})\n",
    "        \"\"\"\n",
    "        df_prod = pd.read_sql(queryp, conn_pg) # type: ignore\n",
    "\n",
    "        df_merged = df_oc.merge(\n",
    "            df_prod,\n",
    "            how='left',\n",
    "            left_on=['c_sucu_empr', 'c_articulo', 'c_proveedor'],\n",
    "            right_on=['c_sucu_empr', 'c_articulo', 'c_proveedor_primario']\n",
    "        )\n",
    "\n",
    "        # Filtrar por cod_cd = '41CD'\n",
    "        df_41= df_merged[df_merged['cod_cd'] == '41CD']\n",
    "        if df_41.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros de cod_cd '41CD' para publicar\")\n",
    "        else:\n",
    "            df_grouped_41 = df_41.groupby(\n",
    "            ['c_proveedor', 'c_articulo'],\n",
    "            as_index=False\n",
    "                ).agg({\n",
    "                    'q_bultos_kilos_diarco': 'sum',\n",
    "                    'f_alta_sist': 'first',\n",
    "                    'c_usuario_genero_oc': 'first',\n",
    "                    'c_terminal_genero_oc': 'first',    \n",
    "                    'f_genero_oc': 'first',\n",
    "                    'c_usuario_bloqueo': 'first',\n",
    "                    'm_procesado': 'first',\n",
    "                    'f_procesado': 'first',\n",
    "                    'u_prefijo_oc': 'first',\n",
    "                    'u_sufijo_oc': 'first',\n",
    "                    'c_compra_kikker': 'first',\n",
    "                    'c_usuario_modif': 'first',\n",
    "                    'c_comprador': 'first'\n",
    "                }).reset_index(drop=True)\n",
    "\n",
    "            df_grouped_41['c_sucu_empr'] = 41\n",
    "            # Borrar en Origen\n",
    "            df_merged.drop(df_merged[df_merged['cod_cd'] == '41CD'].index, inplace=True)\n",
    "            # Publicar en Destino\n",
    "            df_merged = pd.concat([df_merged, df_grouped_41], ignore_index=True)\n",
    "\n",
    "        # Filtrar por cod_cd = '82CD'\n",
    "        df_82= df_merged[df_merged['cod_cd'] == '82CD']\n",
    "        if df_82.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros de cod_cd '82CD' para publicar\")\n",
    "        else:\n",
    "            df_grouped_82 = df_82.groupby(\n",
    "            ['c_proveedor', 'c_articulo'],\n",
    "            as_index=False\n",
    "                ).agg({\n",
    "                    'q_bultos_kilos_diarco': 'sum',\n",
    "                    'f_alta_sist': 'first',\n",
    "                    'c_usuario_genero_oc': 'first',\n",
    "                    'c_terminal_genero_oc': 'first',    \n",
    "                    'f_genero_oc': 'first',\n",
    "                    'c_usuario_bloqueo': 'first',\n",
    "                    'm_procesado': 'first',\n",
    "                    'f_procesado': 'first',\n",
    "                    'u_prefijo_oc': 'first',\n",
    "                    'u_sufijo_oc': 'first',\n",
    "                    'c_compra_kikker': 'first',\n",
    "                    'c_usuario_modif': 'first',\n",
    "                    'c_comprador': 'first'\n",
    "                }).reset_index(drop=True)\n",
    "\n",
    "            df_grouped_82['c_sucu_empr'] = 82\n",
    "            # Borrar en Origen\n",
    "            df_merged.drop(df_merged[df_merged['cod_cd'] == '82CD'].index, inplace=True)\n",
    "            # Publicar en Destino\n",
    "            df_merged = pd.concat([df_merged, df_grouped_82], ignore_index=True)\n",
    "\n",
    "        \n",
    "        # 1C. Traer Stock CENTROS DE DISTRIBUCIÓN EN BULTOS\n",
    "        querystock = f\"\"\"\n",
    "         SELECT S.c_sucu_empr ,S.c_articulo ,S.q_peso_articulo ,P.q_factor_proveedor\n",
    "\t            ,S.q_unid_articulo / p.q_factor_proveedor as stock\n",
    "            FROM src.t060_stock S\n",
    "            LEFT JOIN src.t052_articulos_proveedor P\n",
    "                ON S.c_articulo = P.c_articulo\n",
    "                WHERE P.c_proveedor in ({in_clause}) \n",
    "                and S.c_sucu_empr IN(41, 82)\n",
    "        \"\"\"\n",
    "\n",
    "        df_stock = pd.read_sql(querystock, conn_pg) # type: ignore\n",
    "        if df_stock.empty:\n",
    "            logging.warning(\"[WARNING] No hay stock disponible para los proveedores seleccionados\")\n",
    "        else:\n",
    "            df_stock.rename(columns={'c_sucu_empr': 'c_sucu_empr_stock', 'c_articulo': 'c_articulo_stock'}, inplace=True)\n",
    "            # Hacemos el merge con clave múltiple\n",
    "            # Esto agrega el stock a df_merged\n",
    "            df_merged = df_merged.merge(\n",
    "                df_stock[['c_sucu_empr_stock', 'c_articulo_stock', 'stock']],\n",
    "                how='left',\n",
    "                left_on=['c_sucu_empr', 'c_articulo'],\n",
    "                right_on=['c_sucu_empr_stock', 'c_articulo_stock']\n",
    "            )\n",
    "            # Restar a q_bultos_kilos_diarco stock y tranformar a entero\n",
    "            df_merged['q_bultos_kilos_diarco'] = (\n",
    "                df_merged['q_bultos_kilos_diarco'] - df_merged['stock']\n",
    "                ).clip(lower=0).astype(int) \n",
    "            # Eliminar columnas de stock\n",
    "            df_merged.drop(columns=['c_sucu_empr_stock', 'c_articulo_stock', 'stock'], inplace=True)\n",
    "\n",
    "        conn_pg.close()\n",
    "        return df_merged \n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"[ERROR] Error durante la CONSOLIDACIÓN de OC Precarga\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(\"[ERROR] Error durante la ejecución:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def publicar_oc_precarga():\n",
    "    logging.info(\"[INFO] Iniciando publicación de OC Precarga\")\n",
    "    df_oc = consolidar_oc_precarga()\n",
    "    if df_oc is None or df_oc.empty:\n",
    "        logging.warning(\"[WARNING] No hay registros consolidados para publicar\")\n",
    "        return\n",
    "\n",
    "    # Abrir conexión a PostgreSQL SOLO para el update\n",
    "    conn_pg = Open_Diarco_Data()\n",
    "    if conn_pg is None:\n",
    "        raise ConnectionError(\"[ERROR] No se pudo reconectar a PostgreSQL para actualizar publicados\")\n",
    "\n",
    "    conn_sql = None\n",
    "    cursor_sql = None\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if df_oc.empty:\n",
    "            logging.warning(\"[WARNING] No hay registros pendientes de publicación\")\n",
    "            return\n",
    "\n",
    "        # Agrupar por c_proveedor_primario y c_articulo, sumando las cantidades\n",
    "        total_rows = len(df_oc)\n",
    "        logging.info(f\"[INFO] Registros a publicar: {total_rows}\")\n",
    "\n",
    "        df_oc = limpiar_campos_oc(df_oc)\n",
    "        validar_longitudes(df_oc)\n",
    "        print(df_oc.head(5))\n",
    "\n",
    "        # 2. Conexión a SQL Server\n",
    "        conn_sql = Open_Connection()\n",
    "        if conn_sql is None:\n",
    "            raise ConnectionError(\"[ERROR] No se pudo conectar a SQL Server\")\n",
    "\n",
    "        cursor_sql = conn_sql.cursor()\n",
    "        cursor_sql.fast_executemany = True  # validar si es soportado por tu driver\n",
    "\n",
    "        insert_stmt = \"\"\"\n",
    "        INSERT INTO [dbo].[T080_OC_PRECARGA_KIKKER] (\n",
    "            [C_PROVEEDOR], [C_ARTICULO], [C_SUCU_EMPR], [Q_BULTOS_KILOS_DIARCO],\n",
    "            [F_ALTA_SIST], [C_USUARIO_GENERO_OC], [C_TERMINAL_GENERO_OC], [F_GENERO_OC],\n",
    "            [C_USUARIO_BLOQUEO], [M_PROCESADO], [F_PROCESADO], [U_PREFIJO_OC],\n",
    "            [U_SUFIJO_OC], [C_COMPRA_KIKKER], [C_USUARIO_MODIF], [C_COMPRADOR]\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "\n",
    "        data_tuples = df_oc[[\n",
    "            'c_proveedor', 'c_articulo', 'c_sucu_empr', 'q_bultos_kilos_diarco',\n",
    "            'f_alta_sist', 'c_usuario_genero_oc', 'c_terminal_genero_oc', 'f_genero_oc',\n",
    "            'c_usuario_bloqueo', 'm_procesado', 'f_procesado', 'u_prefijo_oc',\n",
    "            'u_sufijo_oc', 'c_compra_kikker', 'c_usuario_modif', 'c_comprador'\n",
    "        ]].itertuples(index=False, name=None)\n",
    "\n",
    "        cursor_sql.executemany(insert_stmt, list(data_tuples))\n",
    "        conn_sql.commit()\n",
    "\n",
    "        logging.info(\"[INFO] Inserción completada en SQL Server\")\n",
    "        print(f\"✔ Se insertaron {total_rows} registros en SQL Server.\")\n",
    "\n",
    "        # 3. Marcar como publicados en PostgreSQL\n",
    "        lista_compra_kikker = df_oc['c_compra_kikker'].dropna().unique().tolist()\n",
    "        placeholders = ', '.join(['%s'] * len(lista_compra_kikker))\n",
    "        update_stmt = f\"\"\"\n",
    "                UPDATE public.t080_oc_precarga_kikker\n",
    "                SET m_publicado = true\n",
    "                WHERE c_compra_kikker IN ({placeholders})\n",
    "            \"\"\"\n",
    "\n",
    "        with conn_pg.cursor() as cursor_pg:\n",
    "            cursor_pg.execute(update_stmt, lista_compra_kikker)\n",
    "            rows_updated = cursor_pg.rowcount\n",
    "            conn_pg.commit()\n",
    "\n",
    "        logging.info(f\"[INFO] {rows_updated} registros marcados como publicados\")\n",
    "        print(f\"✔ {rows_updated} registros actualizados con m_publicado = true\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"[ERROR] Error durante la publicación de OC Precarga\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(\"[ERROR] Error durante la ejecución:\", e)\n",
    "\n",
    "    finally:\n",
    "        if cursor_sql:\n",
    "            try:\n",
    "                cursor_sql.close()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[WARNING] Error al cerrar cursor SQL: {e}\")\n",
    "        if conn_sql:\n",
    "            try:\n",
    "                conn_sql.close()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[WARNING] Error al cerrar conexión SQL Server: {e}\")\n",
    "        if conn_pg:\n",
    "            try:\n",
    "                conn_pg.close()\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"[WARNING] Error al cerrar conexión PostgreSQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "publicar_oc_precarga()\n",
    "print(f\"[INFO] Proceso finalizado. Ver log en: {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
